{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "affe03fd",
   "metadata": {},
   "source": [
    "## LangChain: Evaluation ##\n",
    "### Outline: ###\n",
    "- Example generation\n",
    "- Manual evaluation (and debuging)\n",
    "- LLM-assisted evaluation\n",
    "- LangChain evaluation platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4f5c1",
   "metadata": {},
   "source": [
    "- **How do you evaluate how well your application is doing? Is it meeting some accuracy criteria?** \n",
    "    - if you decide to change your implementation, maybe swap in a different LLM, or change the strategy of how you use a vector database or something else to retrieve chunks, or change some other parameters of your system, how do you know if you're making it better or worse? \n",
    "    - Let's dive into some frameworks on how to think about evaluating a LLM-based application, as well as some tools to help you do that. \n",
    "    - These applications are really chains and sequences of a lot of different steps. \n",
    "        - understand what exactly is going in and coming out of each step. \n",
    "        - And so some of the tools can really just be thought of as visualizers or debuggers in that vein. \n",
    "        - But it's often really useful to get a more holistic picture on a lot of different data points of how the model is doing. And one way to do that is by looking at things by eye. But \n",
    "    - there's also cool idea of using language models themselves and chains themselves to evaluate other language models, and other chains, and other applications. \n",
    "\n",
    "- **Set up with evaluation.**\n",
    "    - First, we need to have the chain or the application that we're going to evaluate in the first place. \n",
    "    - Second, Use the document question answering chain from the previous lesson. \n",
    "        - So we've got this application, and the first thing we need to do is \n",
    "            - figure out what are some data points that we want to evaluate it on. \n",
    "    - there's a few different methods that we're going to cover for doing this. \n",
    "        - The first is the most simple, which is basically we're going to come up with data points that we think are good examples ourselves. \n",
    "            - To do that, we can just look at some of the data and come up with example questions and then example ground truth answers that we can later use to evaluate. \n",
    "               \n",
    "- The verbose output of `langchain` debug process shows the sequence of steps involved in handling your query, which may appear repetitive at first glance. However, each step represents a distinct stage in the process with a specific purpose. Let's break down these steps to understand their roles:\n",
    "\n",
    "    1. **[chain/start] [1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] Entering Chain run with input:**\n",
    "       - This step signifies the beginning of the entire chain operation. The `RetrievalQA` chain is initiated here.\n",
    "       - The input at this stage is the original question and the **context extracted from the vector store**.\n",
    "           - \"Context\" is the result of a search from the vector store, processed through the StuffDocumentsChain, and then used as input along with your question to generate an answer in the LLMChain\n",
    "       - The chain mentioned here (`RetrievalQA > StuffDocumentsChain > LLMChain`) outlines the sequence of components that will process the query.\n",
    "\n",
    "    2. **[llm/start] [1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] Entering LLM run with input:**\n",
    "       - This step indicates the start of the Large Language Model (LLM) operation within the overall chain.\n",
    "       - The input at this point is the \"prompt\" constructed from the question and the context. This prompt is specifically formatted for the language model (in this case, `ChatOpenAI`).\n",
    "       - The addition of `4:llm:ChatOpenAI` shows that we have moved deeper into the chain, specifically into the language model processing part.\n",
    "\n",
    "    3. **[llm/end] [1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] Exiting LLM run with output:**\n",
    "       - This step marks the completion of the LLM processing.\n",
    "       - The output here is the \"generation\" from the language model, which is the model's response to the input prompt.\n",
    "       - The time `[513.7270000000001ms]` shows how long this LLM processing step took.\n",
    "\n",
    "- the verbose output is detailing each stage of the process, including entering and exiting different components of the chain. Each step represents a different phase:\n",
    "\n",
    "    - **Chain Start**: The whole process begins.\n",
    "    - **LLM Start**: The specific LLM processing starts.\n",
    "    - **LLM End**: The LLM processing ends, and we have the model's output.\n",
    "\n",
    "\n",
    "- And so when doing question answering, oftentimes when a wrong result is returned, it's not necessarily the language model itself that's messing up. It's actually the retrieval step that's messing up. \n",
    " \n",
    "- **Exactly what is entering the language model**, Chat OpenAI itself. \n",
    "    - we can see **the full prompt that's passed in**. \n",
    "        - It is the prompt that the question answering chain is using under the hood,  \n",
    "    - a bunch of the context as inserted before,  \n",
    "    - a human question, which is the question that we asked it. \n",
    "\n",
    "- We can also see a lot more information about the actual return type. \n",
    "    - So rather than just a string, we get back a bunch of information like \n",
    "        - the \"token_usage\", \n",
    "        - the \"prompt_tokens\",\n",
    "        - \"completion_tokens\", \n",
    "        - \"total_tokens\", \n",
    "        - \"model_name\". \n",
    " \n",
    "\n",
    "- what about all the examples we created? How are we going to evaluate those? \n",
    "    - Manual Evaluation \n",
    "        - We could run the chain over all the examples, then look at the outputs, and try to figure out what's going on, whether it's correct, incorrect, partially correct. Similar to creating the examples, that starts to get a little bit tedious over time. \n",
    "\n",
    "    - LLM assisted evaluation\n",
    "        - First, we need to create predictions for all the examples. Before doing that, turn off the debug mode in order to just not print everything out onto the screen. \n",
    "        - I'm going to create predictions for all the different examples. \n",
    "            - we had seven examples total, and so we're going to loop through this chain seven times, getting a prediction for each one. \n",
    "        - Evaluation those examples. \n",
    "            - import the QA question answering eval chain.\n",
    "            - create this chain with a language model, because we're going to be using a language model to help do the evaluation. \n",
    "            - evaluate on this chain. passing in examples and predictions, \n",
    "        - Get back a bunch of graded outputs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8164983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afd52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f741638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8cffc1",
   "metadata": {},
   "source": [
    "### Create our QandA application ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6a96c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.vectorstores import DocArrayInMemorySearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc9ebbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "370b1bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = len(data)\n",
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f676dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e2106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorstoreIndexCreator(\n",
    "    vectorstore_cls=DocArrayInMemorySearch\n",
    ").from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d24a254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0.0, model=llm_model)\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=index.vectorstore.as_retriever(), \n",
    "    verbose=True,\n",
    "    chain_type_kwargs = {\n",
    "        \"document_separator\": \"<<<<>>>>>\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a78ad52",
   "metadata": {},
   "source": [
    "### Coming up with test datapoints ###\n",
    "- what is the datapoints we want to validate on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a1d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eed3b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168eff42",
   "metadata": {},
   "source": [
    "### Hard-coded examples ###\n",
    "- create two examples from above data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32e323cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"Do the Cozy Comfort Pullover Set\\\n",
    "        have side pockets?\",\n",
    "        \"answer\": \"Yes\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What collection is the Ultra-Lofty \\\n",
    "        850 Stretch Down Hooded Jacket from?\",\n",
    "        \"answer\": \"The DownTek collection\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce05ff4",
   "metadata": {},
   "source": [
    "### LLM-Generated examples ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "993cd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QAGenerateChain will take in documents and create quesion answer pair for each documents\n",
    "from langchain.evaluation.qa import QAGenerateChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e9709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa7d84dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create examples\n",
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in data[:5]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142a8ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf6a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a51755",
   "metadata": {},
   "source": [
    "### Combine examples ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a70834",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples += new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "245613a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0.179\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8c14389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Cozy Comfort Pullover Set, Stripe has side pockets on the pull-on pants.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1cf25a",
   "metadata": {},
   "source": [
    "## Manual Evaluation ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01d5c9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15674bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"query\": \"Do the Cozy Comfort Pullover Set        have side pockets?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"question\": \"Do the Cozy Comfort Pullover Set        have side pockets?\",\n",
      "  \"context\": \": 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.<<<<>>>>>: 73\\nname: Cozy Cuddles Knit Pullover Set\\ndescription: Perfect for lounging, this knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out. \\n\\nSize & Fit \\nPants are Favorite Fit: Sits lower on the waist. \\nRelaxed Fit: Our most generous fit sits farthest from the body. \\n\\nFabric & Care \\nIn the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features \\nRelaxed fit top with raglan sleeves and rounded hem. \\nPull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg. \\nImported.<<<<>>>>>: 632\\nname: Cozy Comfort Fleece Pullover\\ndescription: The ultimate sweater fleece \\u2013 made from superior fabric and offered at an unbeatable price. \\n\\nSize & Fit\\nSlightly Fitted: Softly shapes the body. Falls at hip. \\n\\nWhy We Love It\\nOur customers (and employees) love the rugged construction and heritage-inspired styling of our popular Sweater Fleece Pullover and wear it for absolutely everything. From high-intensity activities to everyday tasks, you'll find yourself reaching for it every time.\\n\\nFabric & Care\\nRugged sweater-knit exterior and soft brushed interior for exceptional warmth and comfort. Made from soft, 100% polyester. Machine wash and dry.\\n\\nAdditional Features\\nFeatures our classic Mount Katahdin logo. Snap placket. Front princess seams create a feminine shape. Kangaroo handwarmer pockets. Cuffs and hem reinforced with jersey binding. Imported.\\n\\n \\u2013 Official Supplier to the U.S. Ski Team\\nTHEIR WILL TO WIN, WOVEN RIGHT IN. LEARN MORE<<<<>>>>>: 151\\nname: Cozy Quilted Sweatshirt\\ndescription: Our sweatshirt is an instant classic with its great quilted texture and versatile weight that easily transitions between seasons. With a traditional fit that is relaxed through the chest, sleeve, and waist, this pullover is lightweight enough to be worn most months of the year. The cotton blend fabric is super soft and comfortable, making it the perfect casual layer. To make dressing easy, this sweatshirt also features a snap placket and a heritage-inspired Mt. Katahdin logo patch. For care, machine wash and dry. Imported.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: Use the following pieces of context to answer the users question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n: 10\\nname: Cozy Comfort Pullover Set, Stripe\\ndescription: Perfect for lounging, this striped knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out.\\n\\nSize & Fit\\n- Pants are Favorite Fit: Sits lower on the waist.\\n- Relaxed Fit: Our most generous fit sits farthest from the body.\\n\\nFabric & Care\\n- In the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features\\n- Relaxed fit top with raglan sleeves and rounded hem.\\n- Pull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg.\\n\\nImported.<<<<>>>>>: 73\\nname: Cozy Cuddles Knit Pullover Set\\ndescription: Perfect for lounging, this knit set lives up to its name. We used ultrasoft fabric and an easy design that's as comfortable at bedtime as it is when we have to make a quick run out. \\n\\nSize & Fit \\nPants are Favorite Fit: Sits lower on the waist. \\nRelaxed Fit: Our most generous fit sits farthest from the body. \\n\\nFabric & Care \\nIn the softest blend of 63% polyester, 35% rayon and 2% spandex.\\n\\nAdditional Features \\nRelaxed fit top with raglan sleeves and rounded hem. \\nPull-on pants have a wide elastic waistband and drawstring, side pockets and a modern slim leg. \\nImported.<<<<>>>>>: 632\\nname: Cozy Comfort Fleece Pullover\\ndescription: The ultimate sweater fleece \\u2013 made from superior fabric and offered at an unbeatable price. \\n\\nSize & Fit\\nSlightly Fitted: Softly shapes the body. Falls at hip. \\n\\nWhy We Love It\\nOur customers (and employees) love the rugged construction and heritage-inspired styling of our popular Sweater Fleece Pullover and wear it for absolutely everything. From high-intensity activities to everyday tasks, you'll find yourself reaching for it every time.\\n\\nFabric & Care\\nRugged sweater-knit exterior and soft brushed interior for exceptional warmth and comfort. Made from soft, 100% polyester. Machine wash and dry.\\n\\nAdditional Features\\nFeatures our classic Mount Katahdin logo. Snap placket. Front princess seams create a feminine shape. Kangaroo handwarmer pockets. Cuffs and hem reinforced with jersey binding. Imported.\\n\\n \\u2013 Official Supplier to the U.S. Ski Team\\nTHEIR WILL TO WIN, WOVEN RIGHT IN. LEARN MORE<<<<>>>>>: 151\\nname: Cozy Quilted Sweatshirt\\ndescription: Our sweatshirt is an instant classic with its great quilted texture and versatile weight that easily transitions between seasons. With a traditional fit that is relaxed through the chest, sleeve, and waist, this pullover is lightweight enough to be worn most months of the year. The cotton blend fabric is super soft and comfortable, making it the perfect casual layer. To make dressing easy, this sweatshirt also features a snap placket and a heritage-inspired Mt. Katahdin logo patch. For care, machine wash and dry. Imported.\\nHuman: Do the Cozy Comfort Pullover Set        have side pockets?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain > 4:llm:ChatOpenAI] [513.7270000000001ms] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"The Cozy Comfort Pullover Set, Stripe has side pockets on the pull-on pants.\",\n",
      "        \"generation_info\": null,\n",
      "        \"message\": {\n",
      "          \"content\": \"The Cozy Comfort Pullover Set, Stripe has side pockets on the pull-on pants.\",\n",
      "          \"additional_kwargs\": {},\n",
      "          \"example\": false\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"prompt_tokens\": 734,\n",
      "      \"completion_tokens\": 18,\n",
      "      \"total_tokens\": 752\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo-0301\"\n",
      "  }\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain > 3:chain:LLMChain] [515.444ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"The Cozy Comfort Pullover Set, Stripe has side pockets on the pull-on pants.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA > 2:chain:StuffDocumentsChain] [516.563ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output_text\": \"The Cozy Comfort Pullover Set, Stripe has side pockets on the pull-on pants.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RetrievalQA] [751.17ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"result\": \"The Cozy Comfort Pullover Set, Stripe has side pockets on the pull-on pants.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Cozy Comfort Pullover Set, Stripe has side pockets on the pull-on pants.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa.run(examples[0][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df21b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off the debug mode\n",
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4af6a4",
   "metadata": {},
   "source": [
    "## LLM assisted evaluation ##\n",
    "- First, create predictions for all the different examples. \n",
    "    - we had seven examples total, and so we're going to **loop through this chain seven times**, getting a prediction for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9c484f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "predictions = qa.apply(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314d3c6a",
   "metadata": {},
   "source": [
    "- import the QA question answering eval chain QAEvalChain \n",
    "    - Create this chain with a language model, `we're going to be using a language model to help do the evaluation`. \n",
    "    - Call evaluate on this chain. \n",
    "        - pass in examples and predictions,  \n",
    "    - get back a bunch of graded outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da1cfce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff39f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model)\n",
    "eval_chain = QAEvalChain.from_llm(llm) # llm to do the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1610441",
   "metadata": {},
   "outputs": [],
   "source": [
    "graded_outputs = eval_chain.evaluate(examples, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d1e4d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0:\n",
      "Question: Do the Cozy Comfort Pullover Set        have side pockets?\n",
      "Real Answer: Yes\n",
      "Predicted Answer: The Cozy Comfort Pullover Set, Stripe has side pockets on the pull-on pants.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 1:\n",
      "Question: What collection is the Ultra-Lofty         850 Stretch Down Hooded Jacket from?\n",
      "Real Answer: The DownTek collection\n",
      "Predicted Answer: The Ultra-Lofty 850 Stretch Down Hooded Jacket is from the DownTek collection.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 2:\n",
      "Question: What is the weight of the Women's Campside Oxfords per pair?\n",
      "Real Answer: The Women's Campside Oxfords have an approximate weight of 1 lb. 1 oz. per pair.\n",
      "Predicted Answer: The Women's Campside Oxfords weigh approximately 1 lb. 1 oz. per pair.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 3:\n",
      "Question: What are the dimensions of the small and medium Recycled Waterhog Dog Mat?\n",
      "Real Answer: The dimensions of the small Recycled Waterhog Dog Mat are 18\" x 28\" and the dimensions of the medium Recycled Waterhog Dog Mat are 22.5\" x 34.5\".\n",
      "Predicted Answer: The small Recycled Waterhog Dog Mat has dimensions of 18\" x 28\" and the medium size has dimensions of 22.5\" x 34.5\".\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 4:\n",
      "Question: What are some features of the Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece?\n",
      "Real Answer: The swimsuit features bright colors, ruffles, and exclusive whimsical prints. It is made of four-way-stretch and chlorine-resistant fabric, which keeps its shape and resists snags. The swimsuit is also UPF 50+ rated, providing the highest rated sun protection possible, blocking 98% of the sun's harmful rays. It has crossover no-slip straps and a fully lined bottom to ensure a secure fit and maximum coverage. It can be machine washed and line dried for best results and is imported.\n",
      "Predicted Answer: The Infant and Toddler Girls' Coastal Chill Swimsuit, Two-Piece features bright colors, ruffles, and exclusive whimsical prints. The four-way-stretch and chlorine-resistant fabric keeps its shape and resists snags. The UPF 50+ rated fabric provides the highest rated sun protection possible, blocking 98% of the sun's harmful rays. The crossover no-slip straps and fully lined bottom ensure a secure fit and maximum coverage. It is machine washable and should be line dried for best results.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 5:\n",
      "Question: What is the fabric composition of the Refresh Swimwear, V-Neck Tankini Contrasts?\n",
      "Real Answer: The body of the swimwear is made up of 82% recycled nylon and 18% Lycra® spandex, while the lining is made up of 90% recycled nylon and 10% Lycra® spandex.\n",
      "Predicted Answer: The Refresh Swimwear, V-Neck Tankini Contrasts is made of 82% recycled nylon with 18% Lycra® spandex for the body and 90% recycled nylon with 10% Lycra® spandex for the lining.\n",
      "Predicted Grade: CORRECT\n",
      "\n",
      "Example 6:\n",
      "Question: What is the main feature of the EcoFlex 3L Storm Pants?\n",
      "Real Answer: The EcoFlex 3L Storm Pants are equipped with TEK O2 technology, making them waterproof and highly breathable.\n",
      "Predicted Answer: The main feature of the EcoFlex 3L Storm Pants is the state-of-the-art TEK O2 technology that offers the most breathability ever tested, making them great for a variety of outdoor activities year-round.\n",
      "Predicted Grade: CORRECT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['text'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca9f349",
   "metadata": {},
   "source": [
    "- Why we actually need to use the language model in the first place. \n",
    "    - These two strings:`real answer` and `predicted answer` are actually nothing alike. \n",
    "            - Real Answer: Yes\n",
    "            - Predicted Answer: The Cozy Comfort Pullover Set, Stripe has side pockets on the pull-on pants.\n",
    "        - One's really short, one's really long. \n",
    "        - if we were to try to do some string matching, or exact matching, or even some regexes here, it wouldn't know what to do. They're not the same thing. \n",
    "    - That shows off the importance of using the language model to do evaluation here. \n",
    "        - You've got these answers, which are arbitrary strings. There's no single one truth string that is the best possible answer. \n",
    "        - There's many different variants. And as long as they have the same semantic meaning, they should be graded as being similar. \n",
    "        - And that's what a language model helps with, as opposed to just doing exact matching. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6452e5",
   "metadata": {},
   "source": [
    "### LangChain evaluation platform ###\n",
    "- This is a way to do everything that we just did in the notebook, but persisted and show it in a UI\n",
    "- The LangChain evaluation platform, LangChain Plus, can be accessed here https://www.langchain.plus/. Use the invite code lang_learners_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c89c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
