{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aa90bb1",
   "metadata": {},
   "source": [
    "## LangChain: Memory ##\n",
    "\n",
    "### Outline ###\n",
    "- ConversationBufferMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationTokenBufferMemory\n",
    "- ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb05241",
   "metadata": {},
   "source": [
    "- When you interact with these models, **naturally they don't remember what you say before or any of \n",
    "the previous conversation**, which is an issue when you're building some applications like Chatbot and you want to have a conversation with them. \n",
    "- In this section we'll cover memory, which is basically how do you remember previous parts of the conversation and feed that into the language model so that they can have this conversational flow as you're interacting with them. \n",
    "- LangChain offers multiple sophisticated options of managing these memories. Let's jump in and take a look. \n",
    "\n",
    "### ConversationBufferMemory ###\n",
    "    - using LangChain to manage a chat or a chatbot conversation. \n",
    "        - To do that,set the LLM as a **chat interface of OpenAI with temperature equals 0**, and use the memory as a ConversationBufferMemory, \n",
    "        - build a conversation chain. \n",
    "        - start to have a conversation, conversation.predict, given the input, \n",
    "        - you can change this verbose variable to true, to see what LangChain is actually doing. \n",
    "\n",
    "    - this is what LangChain has remembered in the memory of the conversation so far. It's just everything that the AI or that the human has said. \n",
    "        - the way that LangChain is storing the conversation is with this **ConversationBufferMemory.** \n",
    "        - we can use the ConversationBufferMemory to specify a couple of inputs and outputs, this is how we add new things to the memory if we wish to do it explicitly. \n",
    "        - Now, if we want to add additional data to the memory, you can keep on saving additional context. \n",
    "        \n",
    "- when you use a large language model for a chat conversation, **the large language model itself is actually stateless**. \n",
    "    - The language model itself does not remember the conversation you've had so far. And each transaction, each call to the API endpoint is independent. \n",
    "       - Chatbots appear to have memory only because there's usually rapid code that provides the full conversation that's been had so far as context to the LLM. And so, the memory can store explicitly the terms or the utterances so far. \n",
    "       - And **this memory storage is used as input or additional context to the LLM** so that they can generate an output as if it's just having the next conversational turn, knowing what's been said before. And as the conversation becomes long, the amounts of memory needed becomes really, really long and does the cost of sending a lot of tokens to the LLM, which usually charges based on the number of tokens it needs to process, will also become more expensive. \n",
    "- So LangChain provides several convenient kinds of memory to **store and accumulate the conversation.** \n",
    "    - we've been looking at the ConversationBufferMemory. \n",
    "\n",
    "### ConversationBufferWindowMemory ###\n",
    "\n",
    "- memory that **only keeps a window of memory**. \n",
    "    - If I set memory to conversationalbufferwindowmemory with k=1, the variable k equals one specifies that I wanted to **remember just one conversational exchange**. That is one utterance from user and one utterance from chatbot. \n",
    "        - So, now you were to have it save context, look at memory.load variables, it only remembers the most recent utterance. \n",
    "        - Notice it's dropped.` Hi, what's up?` It's just saying `human says not much, just hanging`, and the AI says cool. that is because k=1. \n",
    "        - it lets you keep track of just the most recent few conversational terms. \n",
    "            - In practice, you probably won't use this with k=1. \n",
    "            - You use this with k set to a larger number. \n",
    "            - But still, this prevents the memory from growing without limit as the conversation goes longer. \n",
    "\n",
    "### ConversationTokenBufferMemory ###\n",
    "- With the conversational token buffer memory, the **memory will limit the number of tokens saved.**\n",
    "    - because a lot of LLM pricing is based on tokens, this maps more directly to the cost of the LLM calls. \n",
    "        - if I were to say the max token limit is equal to 50, \n",
    "        - So let's say the conversation is, \n",
    "            - AI is what? Amazing. \n",
    "            - Backpropagation is what? Beautiful. \n",
    "            - Chatbot is what? Charming. \n",
    "        - I use ABC as the first letter of all of these conversational terms. We can keep track of what was said when. \n",
    "        - If run this with a high token limit, it has almost the whole conversation. \n",
    "        - If increase the token limit to 100, it now has the whole conversation, \n",
    "        - If decrease it, Then, you know, it chops off the earlier parts of this conversation to **retain the number of tokens corresponding to the most recent exchanges** but subject to not exceeding the token limit. \n",
    "    - why we needed to specify an LLM, \n",
    "        it's because **different LLMs use different ways of counting tokens**, so this tells it to use the way of counting tokens that the chatOpenAI LLM uses. \n",
    "\n",
    "### ConversationSummaryMemory ###, \n",
    "- Use an LLM to write a summary of the conversation so far, and let that be the memory. instead of limiting the memory to a fixed number of tokens based on the most recent utterances or a fixed number of conversational exchanges\n",
    "\n",
    "conversation summary buffer memory \n",
    "with a max token limit of 400 in this case, \n",
    "pretty high token limit, \n",
    "and I'm going to \n",
    "insert in \n",
    "a few conversational terms in which we start with hello, what's \n",
    "up, no one's just hanging, \n",
    "cool. \n",
    "And then \n",
    "what is on the schedule today, and the \n",
    "response is, you know, that long schedule. \n",
    "So this \n",
    "memory \n",
    "now has quite a lot of text in it. In fact, let's take a look \n",
    "at the memory variables. \n",
    "It contains that entire \n",
    "piece of text, \n",
    "because 400 tokens was sufficient \n",
    "to store \n",
    "all this text. \n",
    "But now, \n",
    "if I were to reduce the max token limit, say to 100 tokens, \n",
    "remember, this stores the entire conversational history. \n",
    "If I reduce the number of tokens \n",
    "to 100, \n",
    "then, \n",
    "the conversation summary buffer memory has actually used an \n",
    "LLM, the OpenAI endpoint in this case, \n",
    "because that's where we set the LLM to, to \n",
    "actually generate a summary \n",
    "of the conversation so far. So the summary is, human-AI \n",
    "engage in small talk before the start of the schedule, and \n",
    "informs human in the morning meeting, blah, blah, blah, \n",
    "lunch meeting with customer interested in AI and latest \n",
    "AI developments. \n",
    "And so, \n",
    "if we were to have a conversation, \n",
    "using this LLM, \n",
    "let me create a conversation chain, \n",
    "same as before. \n",
    "And let's say that we were to ask, \n",
    "you know, input what would be a good demo to show. \n",
    "I said verbose equals true, so here's the prompt. \n",
    "The LLM thinks the current conversation \n",
    "has had \n",
    "this discussion so far, \n",
    "because that's the summary of the conversation. \n",
    "And just one note, if you're familiar with the OpenAI \n",
    "chat API endpoint, there is a specific system message. \n",
    "In this example, \n",
    "this is not using the official OpenAI system message, \n",
    "it's just including it as part of the prompt here, \n",
    "but it nonetheless works pretty well. And given this prompt, you know, \n",
    "the LLM outputs basic constituent AI developments, \n",
    "so it's just showcasing our latest NLP capabilities. Okay, that's \n",
    "cool. \n",
    "Well, it's, you know, making some suggestions to the cool demos, \n",
    "and makes you think if I was meeting a customer, I \n",
    "would say, \n",
    "boy, if only there were open source framework available to \n",
    "help me build cool NLP applications using LLMs. \n",
    "Good things are launching. \n",
    "And the interesting thing is, if you now look at \n",
    "what has happened to the memory, \n",
    "so, notice that here, it has incorporated the \n",
    "most recent AI system output, whereas my utterance asking it, \n",
    "what would be a good demo to show, has been incorporated \n",
    "into the system message, you know, the overall summary of \n",
    "the conversation so far. With the conversation \n",
    "summary buffer memory, what it tries to do is keep the \n",
    "explicit storage of the messages up to the \n",
    "number of tokens we have specified as a limit. So, \n",
    "you know, this part, the explicit storage, we're \n",
    "trying to cap at 100 tokens, because that's what we're asked for. \n",
    " \n",
    "And then anything beyond that, \n",
    "it will use CLM to generate a summary, which is what is seen up here. \n",
    "And even though I've illustrated these different memories \n",
    "using a chat as a running example, \n",
    "these memories are useful for other applications too, \n",
    "where you might keep on getting new snippets of text, \n",
    "or keep on getting new information, \n",
    "such as if your system repeatedly goes online \n",
    "to search for facts, but you want to keep the total memory \n",
    "used to store this growing list of facts as, you know, \n",
    "capped and not growing arbitrarily long. \n",
    "I encourage you to pause the video and run the code. \n",
    "In this video, you saw a few types of memory \n",
    "including buffer memories that limits based on number of \n",
    "conversation exchanges or tokens \n",
    "or a memory that can summarize tokens above a certain \n",
    "limit. \n",
    "LangChain actually supports additional memory types as well. \n",
    "One of the most powerful is vector data memory, \n",
    "if you're familiar with word embeddings and text embeddings, the \n",
    "vector database actually stores such embeddings. \n",
    "If you don't know what that means, don't worry about it. Harrison will \n",
    "explain it later. \n",
    "And it can then retrieve the most relevant \n",
    "blocks of text using this type of a vector database for its memory. \n",
    "And LangChain also supports entity memories, \n",
    "which is applicable when you wanted to remember \n",
    "details about specific people, specific other entities, \n",
    "such as if you talk about \n",
    "a specific friend, you can have LangChain \n",
    "remember facts about that friend, which would be \n",
    "an entity in an explicit way. When you're implementing applications \n",
    "using LangChain, you can also use multiple types \n",
    "of memories such as \n",
    "using one of the types of conversation memory that you saw in \n",
    "this video plus additionally entity memory \n",
    "to recall individuals. So this way you can remember maybe a summary \n",
    "of the conversation plus an explicit way of storing \n",
    "important facts about important people in \n",
    "the conversation. \n",
    "And of course, in addition to using these memory types, \n",
    "it's also not uncommon for developers to \n",
    "store the entire conversation in the conventional database. Some \n",
    "sort of key value store or SQL database. So you could refer back \n",
    "to the whole conversation for auditing or for \n",
    "improving the system further. \n",
    "And so, that's memory types. I hope you find this useful \n",
    "building your own applications. \n",
    "And now let's go on to the next video to learn about the \n",
    "key building block of LangChain, namely the chain. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146f8316",
   "metadata": {},
   "source": [
    "Note: \n",
    "- LLM's do not always produce the same results. When executing the code in your notebook, you may get slightly different answers that those in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51df6361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa83747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0d90fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5652a35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain #build a conversation chain\n",
    "from langchain.memory import ConversationBufferMemory #use the memory as a ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9fa79de",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True # using verbose=True to see what \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e46be0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Jessica\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello Jessica, it's nice to meet you. My name is AI. How can I assist you today?\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Jessica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b97143ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Jessica\n",
      "AI: Hello Jessica, it's nice to meet you. My name is AI. How can I assist you today?\n",
      "Human: What is 5/6?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5/6 is equivalent to 0.83333 when expressed as a decimal. It is also a proper fraction, meaning the numerator (5) is less than the denominator (6).'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 5/6?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "238a8fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Jessica\n",
      "AI: Hello Jessica, it's nice to meet you. My name is AI. How can I assist you today?\n",
      "Human: What is 5/6?\n",
      "AI: 5/6 is equivalent to 0.83333 when expressed as a decimal. It is also a proper fraction, meaning the numerator (5) is less than the denominator (6).\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Jessica, as you mentioned earlier.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca68bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Jessica\n",
      "AI: Hello Jessica, it's nice to meet you. My name is AI. How can I assist you today?\n",
      "Human: What is 5/6?\n",
      "AI: 5/6 is equivalent to 0.83333 when expressed as a decimal. It is also a proper fraction, meaning the numerator (5) is less than the denominator (6).\n",
      "Human: What is my name?\n",
      "AI: Your name is Jessica, as you mentioned earlier.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82d09875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Jessica\\nAI: Hello Jessica, it's nice to meet you. My name is AI. How can I assist you today?\\nHuman: What is 5/6?\\nAI: 5/6 is equivalent to 0.83333 when expressed as a decimal. It is also a proper fraction, meaning the numerator (5) is less than the denominator (6).\\nHuman: What is my name?\\nAI: Your name is Jessica, as you mentioned earlier.\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The empty dictionary is for more advanced features that can be used with a more sophisticated input \n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ea9e24",
   "metadata": {},
   "source": [
    "- we add new things to the memory if we wish to do it explicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c69a888",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e0bcecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64f08445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31be95e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f6aa286",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b861364e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8369f1",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c55da432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1590926",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4450b5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e07dbfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a796d2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9cf9755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Jessica, it's nice to meet you. My name is AI. How can I assist you today?\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Jessica\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4a27065d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4+1 is equal to 5.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 4+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09e9b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I don't have access to that information. Could you please tell me your name?\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a88f3",
   "metadata": {},
   "source": [
    "### ConversationTokenBufferMemory ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19272bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ce30866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d91cdb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f289ba9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ddb0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase the limit to 100 will buffer all context\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "564590bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!\\nHuman: Backpropagation is what?\\nAI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de9a58c",
   "metadata": {},
   "source": [
    "### ConversationSummaryMemory ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "af03d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c61cde29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72b0a8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments.\"}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88f7509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd030824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the customer's interest in AI developments, I would suggest showcasing our latest natural language processing capabilities. We could demonstrate how our AI can accurately understand and respond to complex language queries, and even provide personalized recommendations based on the user's preferences. Additionally, we could highlight our AI's ability to learn and adapt over time, making it a valuable tool for businesses looking to improve their customer experience.\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b8c28de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI engage in small talk before discussing the day's schedule. The AI informs the human of a morning meeting with the product team, time to work on the LangChain project, and a lunch meeting with a customer interested in the latest AI developments. The human asks what would be a good demo to show.\\nAI: Based on the customer's interest in AI developments, I would suggest showcasing our latest natural language processing capabilities. We could demonstrate how our AI can accurately understand and respond to complex language queries, and even provide personalized recommendations based on the user's preferences. Additionally, we could highlight our AI's ability to learn and adapt over time, making it a valuable tool for businesses looking to improve their customer experience.\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c0e974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
