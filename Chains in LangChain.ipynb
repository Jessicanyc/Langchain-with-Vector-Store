{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "271e1f41",
   "metadata": {},
   "source": [
    "## Chains in LangChain ##\n",
    "\n",
    "- Outline\n",
    "    - LLMChain\n",
    "    - Sequential Chains\n",
    "        - SimpleSequentialChain\n",
    "        - SequentialChain\n",
    "    - Router ChainChains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acada0",
   "metadata": {},
   "source": [
    "\n",
    "#### the key building block of LangChain, namely, the chain. #### \n",
    "- The chain usually **combines an LLM, large language model, together with a prompt**\n",
    "    - with this building block you can also put a bunch of these building blocks together to carry out a sequence of operations on your text or on your other data. \n",
    "        - load some data as Data.csv to load a pandas DataFrame\n",
    "            - there is a product column and then a review column. \n",
    "            - And each of these rows is a different data point that we can start passing through our chains.\n",
    "            - part of the power of these chains is that you can **run them over many inputs at a time**. \n",
    " \n",
    "- **LLM chain**. \n",
    "    - a simple but really powerful chain that underpins a lot of the chains that we'll go over in the future. \n",
    "    - the LLM chain is the most basic type of chain. And that's going to be used a lot in the future. And so we can see how this will be used in the next type of chain\n",
    "    \n",
    "- **SimpleSequentialChains**\n",
    "    - sequential chains run a sequence of chains one after another. where the output of the first chain, is then passed into the second chain. \n",
    "    - The simple sequential chain works well when there's only **a single input and a single output.** \n",
    "    - what about when there are multiple inputs or multiple outputs? \n",
    "- **SequentialChain.**\n",
    "    - create a bunch of chains that we're going to use one after another. \n",
    "    - the fourth chain will take in multiple inputs. \n",
    "        - take in the `summary variable`, which we calculated with the second chain, \n",
    "        - the language variable, which we calculated with the third chain. \n",
    "        - And it's going to ask for a follow-up response to the summary in the specified language. \n",
    "    - the input keys and output keys need to be pretty precise. \n",
    "        - It's really important to get these variable names lined up exactly right, because there's so many different inputs and outputs going on.  \n",
    "    - The simple sequential chain takes in multiple chains, where each one has a single input and a single output. where it has one chain feeding into the other chain, one after another. \n",
    "    - For the sequential chain. Comparing it to the above chain, you can notice that any step in the chain can take in multiple input variables. \n",
    "        - This is useful when you have more complicated downstream chains that need to be a composition of multiple previous chains. \n",
    "\n",
    "- **RouterChain**\n",
    "\n",
    "- Route an input to a chain depending on what exactly that input is. \n",
    "    - A good way to imagine this is if you have multiple sub chains, each of which specialized for a particular type of input, you could have a router chain which \n",
    "        - first decides which subchain to pass it to and then passes it to that chain. \n",
    "        - let's look at where we are routing between different types of chains depending on the subject that seems to come in. \n",
    "            - We have here different prompts. One prompt is good for answering physics questions. \n",
    "            - The second prompt is good for answering math questions, \n",
    "            - the third for history, and then \n",
    "                - a fourth for computer science. \n",
    "        - We can give each one a name and then a description. \n",
    "        - This information is going to be passed to the router chain, so the **router chain can decide when to use this subchain.** \n",
    "        - all the options we have are prompt templates themselves. But this is just one type of thing that you can route between. You can route between any type of chain. \n",
    "    - The other classes that we'll implement here are an **LLM router chain**. \n",
    "        - This **uses a language model itself to route between the different subchains.** \n",
    "        - This is where the description and the name provided above will be used. \n",
    "        - a router output parser parses the LLM output into a dictionary that can be used downstream to determine which chain to use and what the input to that chain should be. \n",
    "        - create the destination chains. These are the chains that will be called by the router chain. \n",
    "        - **each destination chain itself is a language model chain**, an LLM chain. \n",
    "        - In addition to the destination chains, we also need a default chain. \n",
    "            - This is the chain that's called when the router can't decide which of the subchains to use. \n",
    "            - In the example above, this might be called when the input question has nothing to do with physics, math, history, or computer science. \n",
    "    - Now we define the template that is used by the LLM to route between the different chains. This has instructions of the task to be done, as well as the specific formatting that the output should be in. \n",
    "    - Let's put a few of these pieces together to build the router chain. \n",
    "        - First, we create the full router template by **formatting it with the destinations** that we defined above. \n",
    "            - This template is flexible to a bunch of different types of destinations. \n",
    "            - So up here, rather than just physics, math, history, and computer science, you could add a different subject, like English or Latin. \n",
    "            - Next, we create the **prompt template** from this template, and then we create the router chain by passing in the LLM and the overall router prompt. \n",
    "            - Note that here we have the router output parser. \n",
    "                - This is important as it will help this chain decide which subchains to route between. \n",
    "            - And finally, putting it all together, \n",
    "                - we can create the overall chain. \n",
    "            - Now let's ask it some questions. \n",
    "                - If we ask it a question about physics, we should hopefully see that it is routed to the physics chain with the input: \"what is blackbody radiation?\"\n",
    "                    - And then that is passed into the chain down below, and we can see that the response is very detailed with lots of physics details. \n",
    "                - So, for example, if we ask it a math question, we should see that it's routed to the math chain and then passed into that. \n",
    "                - We can also see what happens when we pass in a question that is not related to any of the subchains. So here, we ask it a question about biology and we can see the chain that it chooses is none. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd5e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e4587da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09def22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00f18b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208b410",
   "metadata": {},
   "source": [
    "### LLMChain ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c02b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8f69eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['OPENAI_API_KEY']='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6843dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b94561",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45ab120f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b337858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Queen\\'s Comfort\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d796967",
   "metadata": {},
   "source": [
    "### SimpleSequentialChain ###\n",
    "- the output of the first chain, eg, the company name, is then passed into the second chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "406e6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f3035fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}?\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18ef3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f672feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1efa23cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mRoyal Linens Co.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mRoyal Linens Co. is a manufacturer and retailer of premium quality bedding, towels, and table linens for homes and hotels.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Royal Linens Co. is a manufacturer and retailer of premium quality bedding, towels, and table linens for homes and hotels.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00eb6b",
   "metadata": {},
   "source": [
    "### SequentialChain ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7dee71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d08c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.9, model=llm_model)\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1c77ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ac062a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b73a7a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37bd35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27be88f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       " 'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones from the store and the taste is much better... Old batch or counterfeit!?\",\n",
       " 'summary': 'The reviewer is disappointed with the taste and foam of the product they received, and suspects it may be an old batch or counterfeit.',\n",
       " 'followup_message': \"Résumé : Le commentaire indique que l'auteur est déçu du goût et de la mousse du produit reçu. Il soupçonne qu'il pourrait s'agir d'un lot périmé ou contrefait.\\n\\nRéponse : Nous sommes désolés d'apprendre que vous n'êtes pas entièrement satisfait de votre achat. Nous prenons très au sérieux la qualité de nos produits et nous garantissons que tous nos produits sont frais et authentiques. Si vous pensez que votre produit est périmé ou contrefait, veuillez nous contacter immédiatement pour que nous puissions enquêter sur cette question et trouver une solution pour vous. Nous apprécions votre patronage et nous sommes impatients de vous fournir des produits de la plus haute qualité à l'avenir.\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review = df.Review[5]\n",
    "overall_chain(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9be8465",
   "metadata": {},
   "source": [
    "### Router Chain ###\n",
    "- Route an input to a chain depending on what exactly that input is\n",
    "- A good way to imagine this is if you have multiple sub chains, each of which specialized for a particular type of input, you could have a router chain which \n",
    "    - first decides which subchain to pass it to and then passes it to that chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2f46fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "320bc95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a1d1ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e3feac",
   "metadata": {},
   "source": [
    "- **each destination chain itself is a language model chain**, an LLM chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "398f61a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602885bf",
   "metadata": {},
   "source": [
    "- create the destination chains. These are the **chains that will be called by the router chain**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c70c1f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations) #join string of \"name: description\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ffad9a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'physics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b36ce255",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d21ee49",
   "metadata": {},
   "source": [
    "- define the template that is used by the LLM to route between the different chains. \n",
    "    - This has instructions of the task to be done, \n",
    "    - as well as the specific formatting that the output should be in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6eff75e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: \"destination\" MUST be one of the candidate prompt \\\n",
    "names specified below OR it can be \"DEFAULT\" if the input is not\\\n",
    "well suited for any of the candidate prompts.\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "23868997",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2946ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08b40a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Black body radiation refers to the electromagnetic radiation emitted by a perfect black body, which is an object that absorbs all radiation that falls on it and emits radiation at all wavelengths. The radiation emitted by a black body depends only on its temperature and follows a specific distribution known as Planck's law. This type of radiation is important in understanding the behavior of stars, as well as in the development of technologies such as incandescent light bulbs and infrared cameras.\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8ccb9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As an AI language model, I can answer this question easily. The answer to 2 + 2 is 4.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dae824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
